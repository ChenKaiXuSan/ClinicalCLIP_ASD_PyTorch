# ClinicalCLIP: Clinician-Guided Multimodal Gait Analysis with CLIP

This repository contains the official PyTorch implementation of ClinicalCLIP, a clinician-guided multimodal framework for video-based gait analysis and clinical diagnosis.

The core idea is to integrate clinical prior knowledge (e.g., physician-annotated attention maps) with visionâ€“language pretraining (CLIP) to improve interpretability, robustness, and diagnostic relevance in gait-based analysis.

â¸»

ğŸ” Motivation

Automated gait analysis has shown great potential for non-invasive clinical diagnosis.
However, most existing deep learning approaches:
	â€¢	Treat all spatialâ€“temporal regions equally
	â€¢	Ignore clinically meaningful motion cues
	â€¢	Lack interpretability for medical decision support

In real clinical practice, physicians focus on specific joints, body regions, and motion phases when assessing gait abnormalities.

ClinicalCLIP bridges this gap by aligning gait videos with clinician-guided attention using CLIP-style multimodal learning.

â¸»

âœ¨ Key Contributions
	â€¢	ğŸ§  Clinical Knowledge-Guided Learning
Incorporates clinician-provided attention maps highlighting diagnostically important gait regions.
	â€¢	ğŸ”— CLIP-Based Multimodal Alignment
Aligns gait video representations with clinical attention cues in a shared embedding space.
	â€¢	ğŸ¥ Video-Level Spatiotemporal Modeling
Supports 3D CNN / Transformer-based backbones for robust gait representation learning.
	â€¢	ğŸ” Interpretability by Design
Enables visual and quantitative analysis of where and when the model attends during gait.
	â€¢	ğŸ¥ Non-Invasive Clinical Application
Designed for real-world clinical gait assessment without wearable sensors.

â¸»

ğŸ§© Framework Overview

Input:
  - Gait video (RGB)
  - Clinician-annotated attention maps (spatial / spatiotemporal)

Pipeline:
  Video Encoder (3D CNN / ViT)
        â”‚
        â”œâ”€â”€ Visual Embedding
        â”‚
  Attention Encoder
        â”‚
        â”œâ”€â”€ Clinical Embedding
        â”‚
  â”€â”€â–º CLIP-style Contrastive Alignment
        â”‚
        â””â”€â”€ Downstream Tasks
              â€¢ Diagnosis / Classification
              â€¢ Retrieval
              â€¢ Interpretability Analysis


â¸»

ğŸ“ Repository Structure

ClinicalCLIP/
â”œâ”€â”€ configs/                # Hydra configuration files
â”œâ”€â”€ project/                # Training entrypoints and models
â”‚   â”œâ”€â”€ dataloader/         # Video + attention map datasets
â”‚   â”œâ”€â”€ models/             # CLIP alignment + baselines
â”‚   â””â”€â”€ trainer/            # PyTorch Lightning trainers
â”œâ”€â”€ tests/                  # Unit tests
â””â”€â”€ README.md


â¸»

ğŸš€ Getting Started

1ï¸âƒ£ Environment Setup

conda create -n clinicalclip python=3.10
conda activate clinicalclip
pip install -r requirements.txt

2ï¸âƒ£ Dataset Preparation

Expected data format (for CLIP alignment with clinician attention):

data/
â”œâ”€â”€ videos/
â”‚   â””â”€â”€ subject_x/
â”‚       â””â”€â”€ gait.mp4
â”œâ”€â”€ attention_maps/
â”‚   â””â”€â”€ subject_x/
â”‚       â””â”€â”€ attention.npy
â””â”€â”€ labels.csv

Attention maps can be frame-level, joint-level, or region-level, depending on the experiment.
Set `data.doctor_results_path` and `data.skeleton_path` to enable attention maps.

â¸»

3ï¸âƒ£ Training

python -m project.main \
  train.backbone=clip_align \
  train.attn_map=True \
  model.clip_backbone=3dcnn

Hydra is used for all configurations.

â¸»

ğŸ“Š Evaluation

Supported evaluation settings include:
	â€¢	Diagnosis accuracy / F1-score
	â€¢	Cross-subject validation
	â€¢	Attention consistency analysis
	â€¢	Ablation on clinical priors

âœ… Baseline & backbone comparisons:
  - train.backbone=3dcnn / 2dcnn / cnn_lstm / two_stream
  - CLIP alignment with model.clip_backbone=3dcnn / 2dcnn / cnn_lstm

ğŸ”¬ CLIP implementation notes:
  - Video encoder and attention encoder produce embeddings aligned via contrastive loss.
  - Use model.clip_classifier_source=video/attn/fusion to choose classification head input.
  - Set loss.clip_weight to balance CLIP alignment vs classification loss.

python scripts/eval.py


â¸»

ğŸ“ˆ Visualization

The repository provides tools for:
	â€¢	Attention heatmap overlay on gait videos
	â€¢	Phase-wise gait attention analysis
	â€¢	Case-level interpretability reports

python visualization/vis_attention.py


â¸»

ğŸ¥ Clinical Use Case

This framework is designed for applications such as:
	â€¢	Adult Spinal Deformity (ASD) gait assessment
	â€¢	Neurological disorder screening
	â€¢	Explainable clinical decision support
	â€¢	Human-centered AI in medical video analysis

â¸»

ğŸ“„ Citation

If you find this work useful, please consider citing:

@article{chen2025clinicalclip,
  title   = {ClinicalCLIP: Clinician-Guided Multimodal Gait Analysis via Visionâ€“Language Pretraining},
  author  = {Chen, Kaixu and collaborators},
  journal = {TBD},
  year    = {2025}
}


â¸»

ğŸ“¬ Contact

Kaixu Chen
University of Tsukuba
ğŸ“§ chenkaixusan@gmail.com

â¸»

â­ Acknowledgements

This project is inspired by interdisciplinary collaboration between computer vision researchers and clinicians, aiming to build trustworthy and interpretable medical AI systems.
